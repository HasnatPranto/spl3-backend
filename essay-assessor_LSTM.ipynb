{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b65e7f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from keras.models import Sequential, load_model\n",
    "import keras.backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import cohen_kappa_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "620f6a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"Processed_data.csv\")\n",
    "# df.drop(\"Unnamed: 0\",inplace=True,axis=1)\n",
    "# y = df['final_score']\n",
    "# df.drop('final_score',inplace=True,axis=1)\n",
    "# X=df\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=True)\n",
    "# X_train.shape\n",
    "\n",
    "df = pd.read_csv(\"training_set_rel3.tsv\", sep='\\t', encoding='ISO-8859-1');\n",
    "df.dropna(axis=1,inplace=True)\n",
    "df.drop(columns=['domain1_score','rater1_domain1','rater2_domain1'],inplace=True,axis=1)\n",
    "temp = pd.read_csv(\"Processed_data.csv\")\n",
    "temp.drop(\"Unnamed: 0\",inplace=True,axis=1)\n",
    "df['domain1_score']=temp['final_score']\n",
    "y = df['domain1_score']\n",
    "df.drop('domain1_score',inplace=True,axis=1)\n",
    "X=df\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62fbcc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_e = X_train['essay'].tolist()\n",
    "test_e = X_test['essay'].tolist()\n",
    "train_sents=[]\n",
    "test_sents=[]\n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "def sent2word(x):\n",
    "    x=re.sub(\"[^A-Za-z]\",\" \",x)\n",
    "    x.lower()\n",
    "    filtered_sentence = [] \n",
    "    words=x.split()\n",
    "    for w in words:\n",
    "        if w not in stop_words: \n",
    "            filtered_sentence.append(w)\n",
    "    return filtered_sentence\n",
    "\n",
    "def essay2word(essay):\n",
    "    essay = essay.strip()\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    raw = tokenizer.tokenize(essay)\n",
    "    final_words=[]\n",
    "    for i in raw:\n",
    "        if(len(i)>0):\n",
    "            final_words.append(sent2word(i))\n",
    "    return final_words\n",
    "\n",
    "for i in train_e:\n",
    "    train_sents+=essay2word(i)\n",
    "\n",
    "for i in test_e:\n",
    "    test_sents+=essay2word(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ea1efed",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 300 \n",
    "min_word_count = 40\n",
    "num_workers = 4\n",
    "context = 10\n",
    "downsampling = 1e-3\n",
    "\n",
    "model = Word2Vec(train_sents, \n",
    "                 workers=num_workers, \n",
    "                 vector_size=num_features, \n",
    "                 min_count = min_word_count, \n",
    "                 window = context, \n",
    "                 sample = downsampling)\n",
    "model.wv.save_word2vec_format('word2vecmodel.bin', binary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12c4e446",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hasnat\\AppData\\Local\\Temp\\ipykernel_19504\\2450526373.py:9: RuntimeWarning: invalid value encountered in divide\n",
      "  vec = np.divide(vec,noOfWords)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9083, 300)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def makeVec(words, model, num_features):\n",
    "    vec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    noOfWords = 0.\n",
    "    index2word_set = set(model.wv.index_to_key)\n",
    "    for i in words:\n",
    "        if i in index2word_set:\n",
    "            noOfWords += 1\n",
    "            vec = np.add(vec,model.wv[i])        \n",
    "    vec = np.divide(vec,noOfWords)\n",
    "    return vec\n",
    "\n",
    "\n",
    "def getVecs(essays, model, num_features):\n",
    "    c=0\n",
    "    essay_vecs = np.zeros((len(essays),num_features),dtype=\"float32\")\n",
    "    for i in essays:\n",
    "        essay_vecs[c] = makeVec(i, model, num_features)\n",
    "        c+=1\n",
    "    return essay_vecs\n",
    "\n",
    "\n",
    "clean_train=[]\n",
    "for i in train_e:\n",
    "    clean_train.append(sent2word(i))\n",
    "training_vectors = getVecs(clean_train, model, num_features)\n",
    "\n",
    "clean_test=[] \n",
    "\n",
    "for i in test_e:\n",
    "    clean_test.append(sent2word(i))\n",
    "testing_vectors = getVecs(clean_test, model, num_features)\n",
    "training_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b274749b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(300, dropout=0.4, recurrent_dropout=0.4, input_shape=[1, 300], return_sequences=True))\n",
    "    model.add(LSTM(64, recurrent_dropout=0.4))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='relu'))\n",
    "    model.compile(loss='mean_squared_error', optimizer='rmsprop', metrics=['mse'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86ecc50b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_2 (LSTM)               (None, 1, 300)            721200    \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 64)                93440     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "training_vectors = np.array(training_vectors)\n",
    "testing_vectors = np.array(testing_vectors)\n",
    "\n",
    "# Reshaping train and test vectors to 3 dimensions. (1 represnts one timestep)\n",
    "training_vectors = np.reshape(training_vectors, (training_vectors.shape[0], 1, training_vectors.shape[1]))\n",
    "testing_vectors = np.reshape(testing_vectors, (testing_vectors.shape[0], 1, testing_vectors.shape[1]))\n",
    "lstm_model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab3cdf91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "142/142 [==============================] - 4s 11ms/step - loss: 7.6908 - mse: 7.6908\n",
      "Epoch 2/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 5.2102 - mse: 5.2102\n",
      "Epoch 3/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 4.9624 - mse: 4.9624\n",
      "Epoch 4/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 4.7243 - mse: 4.7243\n",
      "Epoch 5/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 4.5830 - mse: 4.5830\n",
      "Epoch 6/150\n",
      "142/142 [==============================] - 1s 11ms/step - loss: 4.4746 - mse: 4.4746\n",
      "Epoch 7/150\n",
      "142/142 [==============================] - 1s 11ms/step - loss: 4.4217 - mse: 4.4217\n",
      "Epoch 8/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 4.3422 - mse: 4.3422\n",
      "Epoch 9/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 4.2866 - mse: 4.2866\n",
      "Epoch 10/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 4.1978 - mse: 4.1978\n",
      "Epoch 11/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 4.1152 - mse: 4.1152\n",
      "Epoch 12/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 4.0684 - mse: 4.0684\n",
      "Epoch 13/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 4.0016 - mse: 4.0016\n",
      "Epoch 14/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 3.9882 - mse: 3.9882\n",
      "Epoch 15/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 3.8925 - mse: 3.8925\n",
      "Epoch 16/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 3.8525 - mse: 3.8525\n",
      "Epoch 17/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 3.8796 - mse: 3.8796\n",
      "Epoch 18/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 3.7755 - mse: 3.7755\n",
      "Epoch 19/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 3.8253 - mse: 3.8253\n",
      "Epoch 20/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 3.7271 - mse: 3.7271\n",
      "Epoch 21/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 3.7081 - mse: 3.7081\n",
      "Epoch 22/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 3.6615 - mse: 3.6615\n",
      "Epoch 23/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 3.6669 - mse: 3.6669\n",
      "Epoch 24/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 3.6137 - mse: 3.6137\n",
      "Epoch 25/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 3.5466 - mse: 3.5466\n",
      "Epoch 26/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 3.5503 - mse: 3.5503\n",
      "Epoch 27/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 3.5864 - mse: 3.5864\n",
      "Epoch 28/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 3.4582 - mse: 3.4582\n",
      "Epoch 29/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 3.4892 - mse: 3.4892\n",
      "Epoch 30/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 3.4493 - mse: 3.4493\n",
      "Epoch 31/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 3.4052 - mse: 3.4052\n",
      "Epoch 32/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 3.4004 - mse: 3.4004\n",
      "Epoch 33/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 3.3232 - mse: 3.3232\n",
      "Epoch 34/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 3.3373 - mse: 3.3373\n",
      "Epoch 35/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 3.3200 - mse: 3.3200\n",
      "Epoch 36/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 3.3652 - mse: 3.3652\n",
      "Epoch 37/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 3.3091 - mse: 3.3091\n",
      "Epoch 38/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 3.2486 - mse: 3.2486\n",
      "Epoch 39/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 3.2310 - mse: 3.2310\n",
      "Epoch 40/150\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 3.2661 - mse: 3.2661\n",
      "Epoch 41/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 3.2370 - mse: 3.2370\n",
      "Epoch 42/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 3.1338 - mse: 3.1338\n",
      "Epoch 43/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 3.1668 - mse: 3.1668\n",
      "Epoch 44/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 3.1851 - mse: 3.1851\n",
      "Epoch 45/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 3.1587 - mse: 3.1587\n",
      "Epoch 46/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 3.1744 - mse: 3.1744\n",
      "Epoch 47/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 3.1328 - mse: 3.1328\n",
      "Epoch 48/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 3.1066 - mse: 3.1066\n",
      "Epoch 49/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 3.0338 - mse: 3.0338\n",
      "Epoch 50/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 3.0954 - mse: 3.0954\n",
      "Epoch 51/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 3.0601 - mse: 3.0601\n",
      "Epoch 52/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 3.0619 - mse: 3.0619\n",
      "Epoch 53/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 3.0389 - mse: 3.0389\n",
      "Epoch 54/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.9938 - mse: 2.9938\n",
      "Epoch 55/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 3.0414 - mse: 3.0414\n",
      "Epoch 56/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.9641 - mse: 2.9641\n",
      "Epoch 57/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.9826 - mse: 2.9826\n",
      "Epoch 58/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.9170 - mse: 2.9170\n",
      "Epoch 59/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.9492 - mse: 2.9492\n",
      "Epoch 60/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.8951 - mse: 2.8951\n",
      "Epoch 61/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.8823 - mse: 2.8823\n",
      "Epoch 62/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.8996 - mse: 2.8996\n",
      "Epoch 63/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.8849 - mse: 2.8849\n",
      "Epoch 64/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.9283 - mse: 2.9283\n",
      "Epoch 65/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.8839 - mse: 2.8839\n",
      "Epoch 66/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.8348 - mse: 2.8348\n",
      "Epoch 67/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.8314 - mse: 2.8314\n",
      "Epoch 68/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.8268 - mse: 2.8268\n",
      "Epoch 69/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.7950 - mse: 2.7950\n",
      "Epoch 70/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.8080 - mse: 2.8080\n",
      "Epoch 71/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.7795 - mse: 2.7795\n",
      "Epoch 72/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.7615 - mse: 2.7615\n",
      "Epoch 73/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.7402 - mse: 2.7402\n",
      "Epoch 74/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.7660 - mse: 2.7660\n",
      "Epoch 75/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.7301 - mse: 2.7301\n",
      "Epoch 76/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.6958 - mse: 2.6958\n",
      "Epoch 77/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.7753 - mse: 2.7753\n",
      "Epoch 78/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.6691 - mse: 2.6691\n",
      "Epoch 79/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.6758 - mse: 2.6758\n",
      "Epoch 80/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.7215 - mse: 2.7215\n",
      "Epoch 81/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.6912 - mse: 2.6912\n",
      "Epoch 82/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.6459 - mse: 2.6459\n",
      "Epoch 83/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.6386 - mse: 2.6386\n",
      "Epoch 84/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.6468 - mse: 2.6468\n",
      "Epoch 85/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.6283 - mse: 2.6283\n",
      "Epoch 86/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.6295 - mse: 2.6295\n",
      "Epoch 87/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.6049 - mse: 2.6049\n",
      "Epoch 88/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.6330 - mse: 2.6330\n",
      "Epoch 89/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.5621 - mse: 2.5621\n",
      "Epoch 90/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.5681 - mse: 2.5681\n",
      "Epoch 91/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.5972 - mse: 2.5972\n",
      "Epoch 92/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.6235 - mse: 2.6235\n",
      "Epoch 93/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.5693 - mse: 2.5693\n",
      "Epoch 94/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.5240 - mse: 2.5240\n",
      "Epoch 95/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.5347 - mse: 2.5347\n",
      "Epoch 96/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.5547 - mse: 2.5547\n",
      "Epoch 97/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.5361 - mse: 2.5361\n",
      "Epoch 98/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.5052 - mse: 2.5052\n",
      "Epoch 99/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.4660 - mse: 2.4660\n",
      "Epoch 100/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.4728 - mse: 2.4728\n",
      "Epoch 101/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.4881 - mse: 2.4881\n",
      "Epoch 102/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.5110 - mse: 2.5110\n",
      "Epoch 103/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.4649 - mse: 2.4649\n",
      "Epoch 104/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.4814 - mse: 2.4814\n",
      "Epoch 105/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.4841 - mse: 2.4841\n",
      "Epoch 106/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.4722 - mse: 2.4722\n",
      "Epoch 107/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.4748 - mse: 2.4748\n",
      "Epoch 108/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.4262 - mse: 2.4262\n",
      "Epoch 109/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.3998 - mse: 2.3998\n",
      "Epoch 110/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.4249 - mse: 2.4249\n",
      "Epoch 111/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.3977 - mse: 2.3977\n",
      "Epoch 112/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.4358 - mse: 2.4358\n",
      "Epoch 113/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.4034 - mse: 2.4034\n",
      "Epoch 114/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.3998 - mse: 2.3998\n",
      "Epoch 115/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.3823 - mse: 2.3823\n",
      "Epoch 116/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.3791 - mse: 2.3791\n",
      "Epoch 117/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.3462 - mse: 2.3462\n",
      "Epoch 118/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.3772 - mse: 2.3772\n",
      "Epoch 119/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.3416 - mse: 2.3416\n",
      "Epoch 120/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.3448 - mse: 2.3448\n",
      "Epoch 121/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.3703 - mse: 2.3703\n",
      "Epoch 122/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.3329 - mse: 2.3329\n",
      "Epoch 123/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.3510 - mse: 2.3510\n",
      "Epoch 124/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.3113 - mse: 2.3113\n",
      "Epoch 125/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.2912 - mse: 2.2912\n",
      "Epoch 126/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.2887 - mse: 2.2887\n",
      "Epoch 127/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.2764 - mse: 2.2764\n",
      "Epoch 128/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.2861 - mse: 2.2861\n",
      "Epoch 129/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.3008 - mse: 2.3008\n",
      "Epoch 130/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.3129 - mse: 2.3129\n",
      "Epoch 131/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.2858 - mse: 2.2858\n",
      "Epoch 132/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.2802 - mse: 2.2802\n",
      "Epoch 133/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.2416 - mse: 2.2416\n",
      "Epoch 134/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.2393 - mse: 2.2393\n",
      "Epoch 135/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.2363 - mse: 2.2363\n",
      "Epoch 136/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.2260 - mse: 2.2260\n",
      "Epoch 137/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.2162 - mse: 2.2162\n",
      "Epoch 138/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.2124 - mse: 2.2124\n",
      "Epoch 139/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.2120 - mse: 2.2120\n",
      "Epoch 140/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.2056 - mse: 2.2056\n",
      "Epoch 141/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.1978 - mse: 2.1978\n",
      "Epoch 142/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.2127 - mse: 2.2127\n",
      "Epoch 143/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.2074 - mse: 2.2074\n",
      "Epoch 144/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.2084 - mse: 2.2084\n",
      "Epoch 145/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.1702 - mse: 2.1702\n",
      "Epoch 146/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.1615 - mse: 2.1615\n",
      "Epoch 147/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.1818 - mse: 2.1818\n",
      "Epoch 148/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.1465 - mse: 2.1465\n",
      "Epoch 149/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.1398 - mse: 2.1398\n",
      "Epoch 150/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 2.1784 - mse: 2.1784\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1623d34c460>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model.fit(training_vectors, y_train, batch_size=64, epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87ecbbaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122/122 [==============================] - 1s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "lstm_model.save('lstm_model.h5')\n",
    "lstm_model = load_model(\"lstm_model.h5\")\n",
    "y_pred = lstm_model.predict(testing_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e600199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for l in range(5):\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=l, shuffle=True)\n",
    "#     train_e = X_train['essay'].tolist()\n",
    "#     test_e = X_test['essay'].tolist()\n",
    "#     train_sents=[]\n",
    "#     test_sents=[]\n",
    "#     for i in train_e:\n",
    "#         train_sents+=essay2word(i)\n",
    "\n",
    "#     for i in test_e:\n",
    "#         test_sents+=essay2word(i)\n",
    "    \n",
    "#     num_features = 300 \n",
    "#     min_word_count = 40\n",
    "#     num_workers = 8\n",
    "#     context = 10\n",
    "#     downsampling = 1e-3\n",
    "\n",
    "#     model = Word2Vec(train_sents, \n",
    "#                     workers=num_workers, \n",
    "#                     vector_size=num_features, \n",
    "#                     min_count = min_word_count, \n",
    "#                     window = context, \n",
    "#                     sample = downsampling)\n",
    "#     model.wv.save_word2vec_format('word2vecmodel.bin', binary=True)\n",
    "\n",
    "#     clean_train=[]\n",
    "#     for i in train_e:\n",
    "#         clean_train.append(sent2word(i))\n",
    "#     training_vectors = getVecs(clean_train, model, num_features)\n",
    "\n",
    "#     clean_test=[] \n",
    "\n",
    "#     for i in test_e:\n",
    "#         clean_test.append(sent2word(i))\n",
    "#     testing_vectors = getVecs(clean_test, model, num_features)\n",
    "    \n",
    "#     training_vectors = np.array(training_vectors)\n",
    "#     testing_vectors = np.array(testing_vectors)\n",
    "\n",
    "#     training_vectors = np.reshape(training_vectors, (training_vectors.shape[0], 1, training_vectors.shape[1]))\n",
    "#     testing_vectors = np.reshape(testing_vectors, (testing_vectors.shape[0], 1, testing_vectors.shape[1]))\n",
    "#     lstm_model = get_model()\n",
    "#     lstm_model.fit(training_vectors, y_train, batch_size=64, epochs=10)\n",
    "\n",
    "#     if l == 4:\n",
    "#         lstm_model.save_weights('lstm_model.h5')\n",
    "\n",
    "#     y_pred = lstm_model.predict(testing_vectors)\n",
    "#     y_pred = np.around(y_pred)\n",
    "\n",
    "#     y_test_list = y_test.tolist()\n",
    "#     y_pred_list = []\n",
    "\n",
    "#     for i in range(len(y_pred)):\n",
    "#         if str(y_pred[i][0]) == 'nan':\n",
    "#             y_pred[i][0] = 0    \n",
    "#         y_pred_list.append(round(y_pred[i][0]))\n",
    "#     result = cohen_kappa_score(y_test_list, y_pred_list,weights='quadratic')\n",
    "\n",
    "#     print(\"Kappa Score: {}\".format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "722de003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6921650568287518"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "y_test_list = y_test.tolist()\n",
    "y_pred_list = []\n",
    "\n",
    "for i in range(len(y_pred)):\n",
    "    if str(y_pred[i][0]) == 'nan':\n",
    "        y_pred[i][0] = 0    \n",
    "    y_pred_list.append(round(y_pred[i][0]))\n",
    "kappa = cohen_kappa_score(y_test_list, y_pred_list,weights='quadratic')\n",
    "kappa\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "f0a8fbf1307fb74e5848714fa9b66b2a70db2fc19b4d130a0cf84b306bb7724c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
